{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fff9f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fa7b45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/priyotoshsil/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/priyotoshsil/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/priyotoshsil/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/priyotoshsil/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'Dataframe_of_three_measures' on <module '__main__' (built-in)>\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'Dataframe_of_three_measures' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/priyotoshsil/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/priyotoshsil/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'Dataframe_of_three_measures' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/priyotoshsil/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/priyotoshsil/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'Dataframe_of_three_measures' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/priyotoshsil/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/priyotoshsil/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'Dataframe_of_three_measures' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/priyotoshsil/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/priyotoshsil/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'Dataframe_of_three_measures' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mod_sel_BN_for_Derrida import *\n",
    "from BF_properties import *\n",
    "import pickle as pkl\n",
    "import ast\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "def IntsToBitsFast_lr (ints, size):\n",
    "    '''\n",
    "    return an array of bits of length 'size' for the set of integers ints such that the MSB is at column 0\n",
    "    source: https://stackoverflow.com/questions/\n",
    "    ints: numpy array of integers\n",
    "    size: required size of the array of bits\n",
    "    '''\n",
    "    return (((ints[:,None] & (1 << np.arange(size, dtype = 'int64'))[::-1])) > 0).astype(int)\n",
    "\n",
    "def model_data_random(edges, all_nodes, node_num):\n",
    "    '''\n",
    "    edges: It is a dataframe with three columns: 'network_number', 'fro' and 'to' where 'fro' and 'to' corresponds \n",
    "    to vector of nodes from which an edge emanates and terminates respectively.\n",
    "    all_nodes: This is a list of nodes ([gene0, gene1, gene2,...])\n",
    "    node_num: This is a dictionary that links genes with an index ({'gene0': 0, 'gene1': 1, 'gene2': 2, 'gene3': 3,\n",
    "    'gene4': 4, 'gene5': 5, ...})\n",
    "    returns information about the interaction in the network\n",
    "        -in_edges:  a dictionary where each gene is associated with its\n",
    "        regulatory inputs ({0: [11, 1], 1: [8, 6], 2: [10, 11], 3: [0, 11], 4: [5, 11], ....})\n",
    "    '''\n",
    "    in_edges = dict()\n",
    "    for node in all_nodes:\n",
    "        inter = edges.loc[edges[\"to\"] == node, [\"fro\"]]\n",
    "        in_edges[node_num[node]] = [node_num[ele] for ele in inter['fro']]\n",
    "        if in_edges[node_num[node]] == []:\n",
    "            in_edges[node_num[node]] = [node_num[node]]\n",
    "            print (f'Node {node} does not have any inputs. Self loop assigned!')\n",
    "    return in_edges\n",
    "\n",
    "\n",
    "\n",
    "def filter_by_zero_bits(pairs, msb_indices, width):\n",
    "   \n",
    "    mask = 0\n",
    "    for msb_i in msb_indices:\n",
    "        lsb_pos = width - 1 - msb_i\n",
    "        mask |= (1 << lsb_pos)\n",
    "    return [(x, y) for x, y in pairs if (x & mask) == 0 and (y & mask) == 0]\n",
    "\n",
    "\n",
    "# Network sensitivity or Derrida coefficient for a single model\n",
    "def network_sensitivity(inedges, BF_list):\n",
    "    N = len(inedges)\n",
    "    tot_net_sen = 0\n",
    "    for i in range(N):\n",
    "        inp_ith_node = len(inedges[i])\n",
    "        bf_str = bin(BF_list[i])[2:].zfill(2**inp_ith_node)\n",
    "        avg_sen = bf(inp_ith_node, bf_str).avg_sensitivity()\n",
    "        tot_net_sen += avg_sen\n",
    "    ave_net_sen = tot_net_sen/N\n",
    "    return ave_net_sen\n",
    "\n",
    "\n",
    "\n",
    "def final_Hamming_distance(inedges,BF_list,t_0, t_inf, one_neighbors_list):\n",
    "    bio_fp=[]\n",
    "    N = len(inedges)\n",
    "    net_rs = RS(inedges, BF_list, bio_fp)\n",
    "    stv = net_rs.STV()\n",
    "    list_after_evol = one_neighbors_list\n",
    "    sum_ham_array = np.zeros(len(one_neighbors_list))\n",
    "    for step in range(t_inf):\n",
    "        list_after_evol = stv[list_after_evol]\n",
    "        if step >= t_0:\n",
    "            bitwise_xor_result = np.bitwise_xor.reduce(list_after_evol, axis = 1)\n",
    "            ham_dist_list = np.vectorize(lambda x: bin(x).count('1'))(bitwise_xor_result)\n",
    "            sum_ham_array += ham_dist_list\n",
    "    ave_ham_array = sum_ham_array/(t_inf-t_0)\n",
    "    final_Hamming_dist = np.mean(ave_ham_array)\n",
    "    return final_Hamming_dist\n",
    "\n",
    "\n",
    "def get_derrida_coefficient(inedges,BF_list, one_neighbors_list):\n",
    "    N = len(inedges)\n",
    "    bio_fp=[]\n",
    "    net_rs = RS(inedges, BF_list, bio_fp)\n",
    "    stv = net_rs.STV()\n",
    "    list_after_evol = stv[one_neighbors_list]\n",
    "    bitwise_xor_result = np.bitwise_xor.reduce(list_after_evol, axis = 1)\n",
    "    ham_dist_list = np.vectorize(lambda x: bin(x).count('1'))(bitwise_xor_result)\n",
    "    derrida_coeff = np.mean(ham_dist_list)\n",
    "    return derrida_coeff\n",
    "\n",
    "\n",
    "def Fragility(inedges,BF_list,t_0, t_inf, one_neighbors_list):\n",
    "    bio_fp=[]\n",
    "    N = len(inedges)\n",
    "    net_rs = RS(inedges, BF_list, bio_fp)\n",
    "    stv = net_rs.STV()\n",
    "    x_t_vect = IntsToBitsFast_lr (np.zeros(len(one_neighbors_list),dtype=int), N)\n",
    "    x_t_perturb_vect = IntsToBitsFast_lr (np.zeros(len(one_neighbors_list),dtype=int), N)\n",
    "    list_after_evol = list(map(list, zip(*one_neighbors_list)))\n",
    "    for step in range(t_inf):\n",
    "        list_after_evol = stv[list_after_evol]\n",
    "        if step >= t_0:\n",
    "            x_t_vect += IntsToBitsFast_lr (list_after_evol[0], N)\n",
    "            x_t_perturb_vect += IntsToBitsFast_lr (list_after_evol[1], N)\n",
    "    ave_x_t_vect = x_t_vect/(t_inf-t_0)\n",
    "    ave_x_t_perturb_vect = x_t_perturb_vect/(t_inf-t_0)\n",
    "    fragility = np.sum(np.abs(ave_x_t_vect - ave_x_t_perturb_vect), axis=1).mean()\n",
    "    return fragility\n",
    "\n",
    "\n",
    "\n",
    "def append_rows_to_tsv(networks,models,func_types,derrida_coeffs, net_sens, final_hammings, fragilities, file_path, columns):\n",
    "    rows = list(zip(networks,models,func_types,derrida_coeffs, net_sens, final_hammings, fragilities))\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    df.to_csv(file_path, sep='\\t', index=False, mode='a', header=False)\n",
    "\n",
    "\n",
    "\n",
    "def Dataframe_of_three_measures(n,k,func_type,t_0,t_inf,net_init,net_final, model_per_net):\n",
    "    columns = ['Network_no', 'Model_no', 'func_type', 'derrida_coeff', 'net_sen', 'final_hamming', 'fragility']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    file_path = f'../output/{n}_{k}_{func_type}_{net_init}_{net_final}_derrida_PP.tsv'\n",
    "    df.to_csv(file_path, sep='\\t', index=False)\n",
    "    edges_list = pd.read_csv(f'../../RBNs_and_the_models/networks/RN_{n}_{k}_10000_PP.tsv', sep = '\\t') # Contains the interactions for all the networks generated in R (BoolNet)\n",
    "    model_df = pd.read_csv(f'../../RBNs_and_the_models/models_scNCF_IMR_and_BMR/{n}_{k}_10000_models_NCF_IMR_BMR_PP.tsv', sep = '\\t')\n",
    "    with open(f'../input/one_ham_neib_list_for_{n}_nodes_no_repeat.pkl', 'rb') as file:\n",
    "        one_neighbors_list_original = pkl.load(file)\n",
    "\n",
    "    edges_list['fro'] = edges_list['fro'].apply(lambda x: f'gene{x-1}') # We replace the numbers by name: x -> gene{x-1}\n",
    "    edges_list['to'] = edges_list['to'].apply(lambda x: f'gene{x-1}') # for both the 'fro' and 'to' column\n",
    "    edges_list['network_number'] = edges_list['network_number'] - 1\n",
    "    node_num = {'gene'+str(i): i for i in range(n)}\n",
    "    all_nodes = list(node_num.keys())\n",
    "    model_df_relev = model_df[model_df['func_type']==func_type]\n",
    "    net_indices = range(net_init, net_final)\n",
    "    for network_number in net_indices:\n",
    "            networks = [network_number] * model_per_net\n",
    "            models = list(range(1))\n",
    "            func_types = [func_type] * model_per_net\n",
    "            derrida_coeffs, net_sens, final_hammings, fragilities = [], [], [], []\n",
    "            edges = edges_list[edges_list['network_number'] == network_number] # Edges gives the interactions df for a single network\n",
    "            in_edges = model_data_random(edges, all_nodes, node_num)\n",
    "            \n",
    "            models_for_network_df = model_df_relev[model_df_relev['Network_no'] == network_number]\n",
    "            models_for_network_df.loc[:, 'model'] = models_for_network_df['model'].apply(ast.literal_eval)\n",
    "            \n",
    "            if func_type == 'scNCF':\n",
    "                for j in range(model_per_net):\n",
    "                    BF_list = models_for_network_df['model'].iloc[j]\n",
    "                    derrida_coeffs.append(get_derrida_coefficient(in_edges, BF_list,one_neighbors_list_original))\n",
    "                    net_sens.append(network_sensitivity(in_edges, BF_list))\n",
    "                    final_hammings.append(final_Hamming_distance(in_edges,BF_list,t_0, t_inf, one_neighbors_list_original))\n",
    "                    fragilities.append(Fragility(in_edges,BF_list,t_0, t_inf, one_neighbors_list_original))\n",
    "                    \n",
    "            \n",
    "            \n",
    "            if func_type == 'IMR':\n",
    "                for k1 in in_edges:\n",
    "                    if len(in_edges[k1]) % 2 == 0 and k1 not in in_edges[k1]:\n",
    "                        in_edges[k1].append(k1)\n",
    "                        \n",
    "                for j in range(model_per_net):\n",
    "                    BF_list = models_for_network_df['model'].iloc[j]\n",
    "                    derrida_coeffs.append(get_derrida_coefficient(in_edges, BF_list, one_neighbors_list_original))\n",
    "                    net_sens.append(network_sensitivity(in_edges, BF_list))\n",
    "                    final_hammings.append(final_Hamming_distance(in_edges,BF_list,t_0, t_inf, one_neighbors_list_original))\n",
    "                    fragilities.append(Fragility(in_edges,BF_list,t_0, t_inf, one_neighbors_list_original))\n",
    "                \n",
    "                \n",
    "                \n",
    "            if func_type == 'BMR':\n",
    "                for k1 in in_edges:\n",
    "                    if k1 not in in_edges[k1]:\n",
    "                        in_edges[k1].append(k1)\n",
    "                        \n",
    "                for j in range(model_per_net):\n",
    "                    BF_list = models_for_network_df['model'].iloc[j]\n",
    "                    msb_indices = [i for i, val in enumerate(BF_list) if val == 0]\n",
    "                    one_neighbors_list_filtered = filter_by_zero_bits(one_neighbors_list_original, msb_indices, n)\n",
    "                    derrida_coeffs.append(get_derrida_coefficient(in_edges, BF_list, one_neighbors_list_filtered))\n",
    "                    net_sens.append(network_sensitivity(in_edges, BF_list))\n",
    "                    final_hammings.append(final_Hamming_distance(in_edges,BF_list,t_0, t_inf, one_neighbors_list_filtered))\n",
    "                    fragilities.append(Fragility(in_edges,BF_list,t_0, t_inf, one_neighbors_list_filtered))\n",
    "                             \n",
    "            append_rows_to_tsv(networks,models,func_types,derrida_coeffs, net_sens, final_hammings, fragilities, file_path, columns)\n",
    "    \n",
    "\n",
    "\n",
    "def batch(total_networks, total_cores):\n",
    "    networks_per_core = total_networks // total_cores\n",
    "    remaining_networks = total_networks % total_cores\n",
    "    allocations = [networks_per_core] * total_cores\n",
    "    allocations[:remaining_networks] = [x + 1 for x in allocations[:remaining_networks]]\n",
    "    network_init_list = [sum(allocations[:i]) for i in range(len(allocations))]\n",
    "    network_final_list = [sum(allocations[:i]) for i in range(1, len(allocations) + 1)]\n",
    "    return network_init_list, network_final_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n = 12\n",
    "    total_networks = 10\n",
    "    total_cores = 2\n",
    "    for k in range(2,3):\n",
    "        t_0 = 500\n",
    "        t_inf = 700\n",
    "        func_types = ['scNCF', 'IMR', 'BMR']\n",
    "        for func_type in func_types:\n",
    "            model_per_net = 1\n",
    "            net_init_list, net_final_list = batch(total_networks, total_cores)\n",
    "            processes = []\n",
    "            for net_init, net_final in zip(net_init_list, net_final_list):\n",
    "                processes.append(multiprocessing.Process(target=Dataframe_of_three_measures,\n",
    "                                                              args=(n,k,func_type,t_0,t_inf,net_init,net_final, model_per_net)))\n",
    "            for process in processes:\n",
    "                process.start()\n",
    "            for process in processes:\n",
    "                process.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf9a90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
